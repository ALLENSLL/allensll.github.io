<!DOCTYPE html>
<html>
  <head>
    <title>Some popular Gradient-based Optimization Methods – allensll – HomePage</title>

        <meta charset="utf-8" />
    <meta content='text/html; charset=utf-8' http-equiv='Content-Type'>
    <meta http-equiv='X-UA-Compatible' content='IE=edge'>
    <meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1.0'>

    
    <meta name="description" content="Gradient descent is use to optimize deep learning model. In the following, we will outline some methods that are widely used in model training.

" />
    <meta property="og:description" content="Gradient descent is use to optimize deep learning model. In the following, we will outline some methods that are widely used in model training.

" />
    
    <meta name="author" content="allensll" />

    
    <meta property="og:title" content="Some popular Gradient-based Optimization Methods" />
    <meta property="twitter:title" content="Some popular Gradient-based Optimization Methods" />
    

    <!--[if lt IE 9]>
      <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

    <script type="text/javascript" async
      src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>

    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="alternate" type="application/rss+xml" title="allensll - HomePage" href="/feed.xml" />

    <!-- Created with Jekyll Now - http://github.com/barryclark/jekyll-now -->
  </head>
  
  <body>
    <div class="wrapper-masthead">
      <div class="container">
        <header class="masthead clearfix">
          <a href="/" class="site-avatar"><img src="https://raw.githubusercontent.com/allensll/allensll.github.io/master/images/logo.png" /></a>

          <div class="site-info">
            <h1 class="site-name"><a href="/">allensll</a></h1>
            <p class="site-description">HomePage</p>
          </div>

          <nav>
            <a href="/">Blog</a>
            <a href="/links">Links</a>
            <a href="/about">About</a>
          </nav>
        </header>
      </div>
    </div>

    <div id="main" role="main" class="container">
      <article class="post">
  <h2>Some popular Gradient-based Optimization Methods</h2>

  <div class="entry">
    <p>Gradient descent is use to optimize deep learning model. In the following, we will outline some methods that are widely used in model training.</p>

<h3 id="sgd">SGD</h3>

<script type="math/tex; mode=display">\theta_{t+1} = \theta_{t} - \eta \cdot \nabla_{\theta} J\left(\theta_{t}\right)</script>

<h4 id="momentum">Momentum</h4>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned} v_{t} &=\gamma v_{t-1} + \eta \nabla_{\theta}J(\theta_{t}) \\ 
\theta_{t+1} &=\theta_{t} -v_{t} \end{aligned} %]]></script>

<p>The momentum term <script type="math/tex">\gamma</script> is usually set to 0.9.</p>

<h4 id="nesterov-accelerated-gradient">Nesterov Accelerated Gradient</h4>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned} v_{t} &=\gamma v_{t-1}+\eta \nabla_{\theta} J\left(\theta_{t}-\gamma v_{t-1}\right) \\
\theta_{t+1} &=\theta_{t} - v_{t} \end{aligned} %]]></script>

<p>Ref: <a href="http://www.cs.toronto.edu/~hinton/absps/momentum.pdf">On the importance of initialization and momentum in deep learning</a>.</p>

<h3 id="adagrad">Adagrad</h3>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned} g_{t,i} &=\nabla_{\theta} J\left(\theta_{t,i}\right) \\
\theta_{t+1,i} &=\theta_{t,i}-\frac{\eta}{\sqrt{\sum_{t=1}^t{g_{t,i}^2}+\epsilon}} g_{t,i} \end{aligned} %]]></script>

<p>Ref: <a href="http://www.jmlr.org/papers/volume12/duchi11a.html">Adaptive Subgradient Methods for Online Learning and Stochastic Optimization</a>.</p>

<h3 id="adadelta">Adadelta</h3>

<p>use <script type="math/tex">R M S[\Delta \theta]_{t-1}</script> to approximate <script type="math/tex">\Delta_{t}</script>, multiply <script type="math/tex">\Delta_{t}</script> to correct units.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned} E\left[\Delta \theta^{2}\right]_{t} &=\gamma E\left[\Delta \theta^{2}\right]_{t-1}+(1-\gamma) \Delta \theta_{t}^{2} \\
R M S[\Delta \theta]_{t} &=\sqrt{E\left[\Delta \theta^{2}\right]_{t}+\epsilon} \\
\theta_{t+1} &=\theta_{t}- \frac{R M S[\Delta \theta]_{t-1}}{R M S[g]_{t}} g_{t} \end{aligned} %]]></script>

<p>Ref: <a href="https://arxiv.org/abs/1212.5701">ADADELTA: An Adaptive Learning Rate Method</a>.</p>

<h3 id="rmsprop">RMSprop</h3>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned} E\left[g^{2}\right]_{t} &=\gamma E\left[g^{2}\right]_{t-1}+\left(1-\gamma \right) g_{t}^{2} \\ 
\theta_{t+1} &=\theta_{t}-\frac{\eta}{\sqrt{E\left[g^{2}\right]_{t}+\epsilon}} g_{t} \end{aligned} %]]></script>

<p>Ref: <a href="http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf">G.Hinton’s course</a>.</p>

<h3 id="adam">Adam</h3>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned} m_{t} &=\beta_{1} m_{t-1}+\left(1-\beta_{1}\right) g_{t} \\ v_{t} &=\beta_{2} v_{t-1}+\left(1-\beta_{2}\right) g_{t}^{2} \\
\hat{m}_{t} &=\frac{m_{t}}{1-\beta_{1}^{t}} \\ \hat{v}_{t} &=\frac{v_{t}}{1-\beta_{2}^{t}} \\
\theta_{t+1} &=\theta_{t}-\frac{\eta}{\sqrt{\hat{v}_{t}}+\epsilon} \hat{m}_{t} \end{aligned} %]]></script>

<p>Ref: <a href="https://arxiv.org/abs/1412.6980">Adam: A Method for Stochastic Optimization</a>.</p>

<h4 id="external-links"><em>External links</em></h4>
<ul>
  <li><a href="http://ruder.io/optimizing-gradient-descent/index.html#gradientdescentvariants">An overview of gradient descent optimization algorithms</a></li>
</ul>

<p><br /></p>

  </div>

  <div class="date">
    Written on May 22, 2019
  </div>

  
</article>

    </div>

    <div class="wrapper-footer">
      <div class="container">
        <footer class="footer">
          <a href="https://dribbble.com/sunll90"><i class="svg-icon dribbble"></i></a>



<a href="https://github.com/allensll"><i class="svg-icon github"></i></a>




<a href="https://www.twitter.com/sunll90"><i class="svg-icon twitter"></i></a>




        <footer class="footer">
          © Copyright 2017 allensll. Powered by <a href="http://www.jekyllnow.com">Jekyll Now</a>. Last updated: March 25, 2018.
        </footer>
      </div>
    </div>
    

  </body>
</html>
